---
title: "Chapter 4 - Linear Models"
output: html_notebook
---

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(skimr)
library(rethinking)
```

# Notes

## Introduction
Linear regression describes a measurements mean and variance as the addition of other measurements. It assumes the errors in the primary measurement are of the gaussian distribution.

## Normality

1000 individuals conduct a random walk on either side of the football field starting at the 50 yard line (value = 0). This signifies the normal distribution.
```{r fig.width=14}
INDIVIDUALS <- 1000
TRIALS <- 100

purrr::map(1:INDIVIDUALS, ~ runif(TRIALS, -1, 1)) %>% 
  purrr::map(cumsum) %>% 
  reduce(rbind) %>% 
  as_tibble %>% 
  `colnames<-`(paste0(1:TRIALS, "t")) %>% 
  mutate(individual = row_number()) %>% 
  reshape2::melt("individual") %>% 
  arrange(individual, variable) %>% 
  ggplot(aes(x=variable, y=value, group=individual)) + 
    geom_line(alpha = I(1/20), size=1) +
    geom_boxplot(aes(x=variable, y = value), inherit.aes = FALSE, colour = "yellow", alpha= I(1/40))
```

The above example shows us that normality is additive. 
```{r}
# Generate 12 numbers between 1 and 1.1 then add them in to one sum
# Do this 1000 times and collect the numbers in a vector
growth <- replicate(1000, sum(runif(12,0,.1)))
# Plot the distribution of thes enumbers. Note that it is normal.
qplot(growth)
```

We can also show that normality is multiplicative.
```{r}
# Generate 12 numbers between 1 and 1.1 then multiply them in to one product
# Do this 1000 times and collect the numbers in a vector
growth <- replicate(1000, prod(1 + runif(12,0,.1)))
#plot the distribution of these numbers. Note that it is normal.
qplot(INDIVIDUALS)
```

Things start to skew at very large deviates, though this can be corrected by log scaling.
```{r }
big <- replicate(10000, prod(1 + runif(12, 0, .5)))
small <- replicate(10000, prod(1 + runif(12, 0, 0.01)))

data_frame(big = big, 
           small = small, 
           log_big = log(big)) %>% 
  gather %>% 
  ggplot(aes(value)) + 
    geom_histogram() + 
    facet_wrap("key", scales = "free")
```

## A language for describing models

Elements:

1. **An outcome variable**: The set of measurements we hope to predict or understand
2. **Likeliehood distribution**: For each of these outcome variables, we define the plausibility of that observations.
3. **Predictor variables**: Set of other measurements that we hope to use to predict or understand the outcome.

Approach:
1. Relate the exact shape of the likelihood distribution to the predictor variables via parameters
2. Choose priors for all the parameters in the model.


## Example: A gaussian model of height

We'll build a regression to predict a human's hight, which we will model as a Gaussian distribution with two parameters, mean and standard deviation. There are an infinite number of possible Gaussian distributions. We want our Bayesian machine to consider every possible distribution, each defined by a combination of meand and sd, and rank them by posterior plausibility given the data.

```{r}
library(pander)
data(Howell1)
d <- Howell1
d2 <- d %>% 
  filter(age >= 18)
d2 %>% 
  skim %>% 
  pander
```

Lets specifiy our model like so. The first line is the likelihood of our outcome, height. The second two lines are priors for the parameters in our likelihood.
$$
h_i \sim Normal(\mu, \sigma) \\
\mu \sim Normal(178, 20) \\
\sigma \sim Uniform(0, 50)
$$

As height is a physical phenomenon, we can use our intuition to assign a prior.
```{r}
curve(dnorm(x, 178, 20), from=100, to=250)
```

We give the standard deviation a truly flat prior. We really just want to make sure its positive.
```{r}
curve(dunif(x, 0, 50), from=-10, to=60)
```

Specifying a prior for the parameters of our outcome implicitly give us a prior for our outcome.
```{r}
sample_mu <- rnorm(1e4, 178, 20)
sample_sigma <- runif(1e4, 0, 50)
prior_h <- rnorm(1e4, sample_mu, sample_sigma)
qplot(prior_h)
```

Lets us grid approximation to generate the posterior. To do so, we enumerate a list of candidate values for our parameters and store them in post. We then calculate the log likelihood for each of those parameters given all the heights we have seen and store them in the column LL. We use log transformation to avoid rounding errors. Then we multiply by the priors (log addition is the same as multiplying)
```{r}
# Generate candidate parameters
mu_list <- seq(from=140, to=160, length.out=200)
sigma_list <- seq(from=4, to=9, length.out=200)
post <- expand.grid(mu=mu_list, sigma=sigma_list)
# Calculate likelihood
post$LL <- sapply(1:nrow(post), function(i) {
  sum(dnorm(
    d2$height,
    mean=post$mu[i],
    sd=post$sigma[i],
    log=TRUE))})
# Multiply priors
post$prod <- post$LL + dnorm(post$mu, 178, 20, TRUE) +
  dunif(post$sigma, 0, 50, TRUE)
# Calculated scaled posterior
post$prob <- exp(post$prod - max(post$prod))
# Contour plot of probabilities.
ggplot(post, aes(x=mu, y=sigma, z=prob)) + 
  geom_contour(aes(colour=stat(level))) + 
  coord_cartesian(xlim=c(140, 160), ylim=c(4, 9))
```

Now lets sample from the posterior.
```{r}
# Sample candidate values by posterior
sample_rows <- sample(1:nrow(post), size=1e4, replace=TRUE, prob=post$prob)
# Plot their 2d histogram
data_frame(
  sample_mu = post$mu[sample_rows],
  sample_sigma = post$sigma[sample_rows]
) %>% 
  ggplot(aes(sample_mu, sample_sigma)) +
  geom_bin2d(bins=30) + 
  coord_cartesian(xlim=c(140, 160), ylim=c(4, 9))
```

Lets also examine the margins.
```{r}
post$mu[sample_rows] %>% 
  qplot
post$sigma[sample_rows] %>% 
  qplot

```

`map` from the rethinking package allows us to solve for the posterior using quadratic approximation instead of grid approximations. It shows us gaussian approximations for each parameter's marginal distribution.
```{r}
flist <- alist(
  height ~ dnorm(mu, sigma),
  mu ~ dnorm(178, 20),
  sigma ~ dunif(0, 50)
)
m4.1 <- map(flist, data=d2)
precis(m4.1)
```

Variance covariance matrix tells us how each parameter relates to every other parameter in the posterior distribution.
```{r}
vcov(m4.1)
# This can be factored in to a vector of variances
diag(vcov(m4.1))
# Or a correlation matrix
cov2cor(vcov(m4.1))
```

Lets draw from the posterior by sampling vectors of values from a multi-dimensional gaussian distribution.
```{r}
post <- extract.samples(m4.1, n=1e4)
precis(post)
```

### Adding a predictor variable, weight

It looks like weight will be a good predictor for height in adults.
```{r}
ggplot(d2, aes(x=weight, y=height)) + 
  geom_point(shape=1) +
  geom_smooth(method=lm)
```

The goal here is to make the parameter for the mean of a Gaussian distribution, $\mu$, into a linear function of the predictor variable. This encodes the assumption that the predictor variable has a perfectly constant and additive relationship to the mean of the outcome.
```{r}
m4.3 <- map(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b*weight,
    a ~ dnorm(156, 100),
    b ~ dnorm(0, 10),
    sigma ~ dunif(0, 50)),
  data = d2)
# Display table of estimates
precis(m4.3, corr=TRUE)
```

The table above tells us that a person 1 kg heavier is expected to be .9 cm taller. It also tells us that a person with 0 weight is 114 cm tall, which is of course nonsense; This is why it is usually important to have very weak priors for intercepts in many cases. Finally, it explains that 95% of plausible heights lie within 10 cm of the mean.

Centering is the procedure of subtracting the mean of a variable from each value. 
```{r}
d2$weight.c <- d2$weight - mean(d2$weight)

m4.4 <- map(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b*weight.c,
    a ~ dnorm(178, 100),
    b ~ dnorm(0, 10),
    sigma ~ dunif(0, 50)),
  data = d2)
# Display table of estimates
precis(m4.4, corr=TRUE)
```

By **centering** our predictor variable, the interpretation of the intercept has now become the expected value of the outcome when the predictor is at its average value. This makes interpreting the intercept a lot easier. 

### Visualizing our estimates

We can plot our maximum a posteriori fit here

```{r}
p <- ggplot(d2, aes(x=weight, y=height)) +
  geom_point(shape=1, size=2) +
  geom_abline(aes(intercept = coef(m4.3)["a"],
                  slope = coef(m4.3)["b"]))
p
```

This line only represents one piece of the posterior. We could instead sample values of $\alpha$ and $\beta$ from the posterior to generate many lines to build bounds of uncertainty. We'll do this with a much smaller dataset to magnify effects.
```{r}
N <- 10
dN <- d2[ 1:N , ]
mN <- map(
  alist(
    height ~ dnorm( mu , sigma ) ,
    mu <- a + b*weight ,
    a ~ dnorm( 178 , 100 ) ,
    b ~ dnorm( 0 , 10 ) ,
    sigma ~ dunif( 0 , 50 )
) , data=dN )

post <- extract.samples(mN, n=20)
head(post)
```

```{r}
p <- ggplot(dN, aes(weight, height)) +
  geom_point(shape=1)

for (i in 1:20) {
  p <- p + geom_abline(intercept = post$a[i], slope = post$b[i], alpha = I(3/10))
}

p
```

To do this more generally:

1. Sample parameter values from the posterior.
2. Use the parameter values to create predictions of the parameters of interest, including the final outcome.
3. Use summary functions like mean, HDOP, PI to find averages and lower and upper bounds of our uncertainty.


```{r}
# Draw parameter values from the posterior distribution
posterior_samples <- extract.samples(m4.3)

# Create out x-axis of interest
weights_axis <- 25:70

# Function to calculate fit values for \mu
mu_link <- function(weight) post$a + post$b*weight
mu <- purrr::map(weights_axis, mu_link)

# Function to calculate fit heights
# This incorporates variability as well
simulate <- function(weight, parameter_samples) {
  rnorm(n = nrow(parameter_samples),
        mean = parameter_samples$a + parameter_samples$b * weight,
        sd = parameter_samples$sigma)}
simulated_heights <- purrr::map(weights_axis, ~ simulate(.x, posterior_samples))

# Combine all this information to plot
data_frame(
  # X axis
  weight = weights_axis,
  # Estimates for mean height
  mu_mean = map_dbl(mu, mean),
  mu_lower = map_dbl(mu, ~HPDI(.x, .89)[1]),
  mu_upper = map_dbl(mu, ~HPDI(.x, .89)[2]),
  # Estimates for height
  height_mean = map_dbl(simulated_heights, mean),
  height_low = map_dbl(simulated_heights, ~HPDI(.x, .89)[1]),
  height_high = map_dbl(simulated_heights, ~HPDI(.x, .89)[2])) %>% 
  ggplot(data=.) +
  geom_point(inherit.aes = FALSE, data = d2,
             aes(x=weight, y=height), shape=1) +
  geom_line(aes(x=weight, y=mu.mean)) +
  geom_ribbon(aes(x = weight, ymin=mu_lower, ymax=mu_upper), alpha=.5) +
  geom_ribbon(aes(x = weight, ymin = height_low, ymax = height_high), alpha=.3) +
  ggtitle("The effect of weight on height", subtitle = "Using 89% highest posterior density intervals") +
  coord_cartesian(xlim=c(30, 60),ylim=c(135, 180))
```

## Polynomial Regression

Standardizing our predictors means scaling but also dividing by the standard deviation. This changes our interpretation of continuous variables to mean the unit change in outcome from a cahnge of one standard deviation. 

```{r}
# Standardize continuous variable
standardize <- function(vec) {
  centered <- vec - mean(vec)
  centered / sd(vec)
}
d$weight.s <- standardize(d$weight)

# Define another polynomial term
d$weight.s2 <- d$weight.s^2

m4.5 <- map(
  alist(
    # Outcome
    height ~ dnorm(mu, sigma),
    # Regression specification
    mu <- a + b1*weight.s + b2*weight.s2,
    a ~ dnorm(178 , 100),
    # These priors are very weak!
    b1 ~ dnorm(0 , 10),
    b2 ~ dnorm(0 , 10),
    sigma ~ dunif(0 , 50)),
  data=d )
precis( m4.5 )
```

# Homework

## Medium
### 4M1
For the model definition below, simulate observed heights from the prior.
```{r}
num_observations <- 1e4

prior_mean <- rnorm(num_observations, 0, 10)
prior_sd <- runif(num_observations, 0, 10)

prior_heights <- rnorm(num_observations, prior_mean, prior_sd)

qplot(prior_heights)
```


### 4M2
```{r}
model <- alist(
  y ~ dnorm(mu, sigma),
  mu ~ dnorm(0, 10),
  sigma ~ dunif(0, 10)
)
```



## Hard
### 4H1
```{r}
data("Howell1")
d <- Howell1
d$weight <- standardize(d$weight)
d
```


```{r}
model <- map(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b * weight,
    a ~ dnorm(mean = 0, sd = 100),
    b ~ dnorm(mean = 0, sd = 10),
    sigma ~ dunif(min = 0, max = 64)
  ),
  data = d
)
model
```

```{r}
# Standardize our input
new_weights <- standardize(c(46.95, 43.72, 64.78, 32.59, 54.63))

# Draw parameter values from the posterior distribution
posterior_samples <- extract.samples(model)

# Function to calculate fit heights
simulate_heights <- function(weight, parameter_samples) {
  rnorm(
    n = nrow(parameter_samples),
    mean = parameter_samples$a + parameter_samples$b * weight,
    sd = parameter_samples$sigma)}

# Generate a prediction for each new weight
# Use every parameter set in the posterior to do so
simulated_heights <- purrr::map(new_weights, ~ simulate(.x, posterior_samples))

data_frame(
  individual = 1:5,
  weight = c(46.95, 43.72, 64.78, 32.59, 54.63),
  expected_height = map_dbl(simulated_heights, mean),
  lower_pi = map_dbl(simulated_heights, ~PI(.x)[1]),
  upper_pi = map_dbl(simulated_heights, ~PI(.x)[2])
)
```

# 4H2
```{r}
data("Howell1")
d <- Howell1
d %>% 
  filter(age < 18) %>% 
  ggplot(aes(x = weight, y = height)) +
  geom_point() + geom_smooth(method=lm) +
  geom_smooth(colour = "orange")
```

```{r}
d %>% 
  filter(age < 18) %>% 
  map(alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b * weight,
    a ~ dnorm(mean = 100, sd = 100),
    b ~ dnorm(mean = 0, sd = 10),
    sigma ~ dunif(min = 0, max = 50)
  ),
  data = .) ->
model

precis(model)
```

A one unit increase in weight increases height by 2.72, therefore a 10 unit increase will move height by 27.2.

```{r}
# Sample the poseterior
posterior_samples <- extract.samples(model)

# Create x-axis
new_weights <- seq(from = 4, to = 45, length.out = 100)

# Calculate the mean
fn_mean <- function(weight) {posterior_samples$a + posterior_samples$b * weight}
fit_means <- purrr::map(new_weights, fn_mean)

# Calculate heights
fn_height <- function(weight) {
  rnorm(
    n = nrow(posterior_samples),
    mean = posterior_samples$a + posterior_samples$b * weight,
    sd = posterior_samples$sigma)
}
fit_heights <- purrr::map(new_weights, fn_height)

data_frame(
  weights = new_weights,
  means = map_dbl(fit_means, mean),
  means_low = map_dbl(fit_means, ~ HPDI(.x, .89)[1]),
  means_high = map_dbl(fit_means, ~ HPDI(.x, .89)[2]),
  heights = map_dbl(fit_heights, mean),
  heights_low = map_dbl(fit_heights, ~ HPDI(.x, .89)[1]),
  heights_high = map_dbl(fit_heights, ~ HPDI(.x, .89)[2]),
) %>% 
ggplot(aes(x = weights)) +
  geom_line(aes(y = means)) +
  geom_ribbon(aes(ymin = means_low, ymax = means_high), alpha = .4) +
  geom_ribbon(aes(ymin = heights_low, ymax = heights_high), alpha = .2) +
  geom_point(inherit.aes = FALSE, data = filter(d, age < 18),
             aes(x=weight, y = height), shape = 1) +
  ylab("height") + xlab("weight") + theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
```


