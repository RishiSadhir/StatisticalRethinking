---
title: "Chapter 14 - Adventures in Covariance"
output: html_notebook
---

```{r setup}
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(brms))
suppressPackageStartupMessages(library(tidybayes))

options(mc.cores = parallel::detectCores())
```

# Varying slopes by construction

In the same way that pooling is helpful for intercepts, it is also helpful for slopes. Any batch of parameters with exchangeable index values (index values have no true ordering because they are arbitrary) can and should be pooled.

$$
Q \sim Binomial(p, D)
\\
p = \alpha_{sku[i]} + \beta_{sku[i]}Z_i
$$

Clustering can affect many parameters at once! A sku's class affects both its base line sale rate ($\alpha_{sku[i]}$) and its marginal flagged sale rate ($\beta_{sku[i]}$).

Clusters affect both intercepts and slopes. We account for this by modeling the joint population of intercepts and slopes by modeling their covariance through multivariate gaussian distributions.

Simulated example - Cafes are busy in the morning but maybe not the afternoon. We'll define a covariance matrix between a cafe's intercept and slope.
```{r}
a       <-  3.5  # average morning wait time
b       <- -1    # average difference afternoon wait time
sigma_a <-  1    # std dev in intercepts
sigma_b <-  0.5  # std dev in slopes
rho     <- -.7   # correlation between intercepts and slopes

# The next three lines of code simply combine the terms, above
mu <- c(a, b)

cov_ab <- sigma_a * sigma_b * rho
sigma  <- matrix(c(sigma_a^2, cov_ab, 
                   cov_ab, sigma_b^2), ncol = 2)

```

We can also get there via matrix multiplication. This way is conceptually more useful when it comes time to specify priors because it lets you define standard deviations and correlations seperately.

```{r}
# Standard deviations
sigmas <- c(sigma_a, sigma_b)
# Correlations
Rho    <- matrix(c(1, rho, rho, 1), nrow = 2)
# Matrix multiply to get vcov matrix
sigma  <- diag(sigmas) %*% Rho %*% diag(sigmas)

set.seed(5)
n_cafes <- 200
vary_effects <- 
  # Feed in means and covariances
  # in to a multivariate gaussian rng
  MASS::mvrnorm(n_cafes, mu, sigma)

vary_effects <-
  vary_effects %>% 
  as_tibble() %>% 
  rename(a_cafe = V1,
         b_cafe = V2)

str(vary_effects)
```

```{r}
vary_effects %>% 
  ggplot(aes(x = a_cafe, y = b_cafe)) +
  geom_smooth(method = lm, colour = "steelblue") +
  geom_point(color = "steelblue") +
  geom_rug(alpha = I(1/6)) +
  theme(panel.grid = element_blank(),
        panel.background = element_rect(fill = "transparent", colour = "black")) +
  ggtitle("Distribution of parameters")
```

Lets use this covariance relationship to simulate wait time ovservations at two cafes.
```{r}
n_visits <- 10
sigma    <-  0.5  # std dev within cafes
set.seed(5)  # used to replicate example

d <- vary_effects %>%
  # Create cafe index
  mutate(cafe      = 1:n_cafes) %>% 
  # n_visits for each one
  expand(nesting(cafe, a_cafe, b_cafe), visit = 1:n_visits) %>% 
  # Split them between morning and afternoon
  mutate(afternoon = rep(0:1, times = n() / 2)) %>% 
  # Calculate the mean
  mutate(mu        = a_cafe + b_cafe * afternoon) %>% 
  # Simulat the wait time
  mutate(wait      = rnorm(n = n(), mean = mu, sd = sigma))

d %>%
  mutate(afternoon = ifelse(afternoon == 0, "M", "A"),
         day       = rep(rep(1:5, each = 2), times = n_cafes)) %>%
  filter(cafe %in% c(3, 5)) %>%
  mutate(cafe = ifelse(cafe == 3, "cafe #3", "cafe #5")) %>%
  
  ggplot(aes(x = visit, y = wait, group = day)) +
  geom_line(color = "#8B9DAF") +
  geom_point(aes(color = afternoon), size = 3) +
  scale_color_manual(values = c("#80A0C7", "#EEDA9D")) +
  scale_x_continuous(breaks = 1:10,
                     labels = rep(c("M", "A"), times = 5)) +
  coord_cartesian(ylim = 0:8) +
  labs(x = NULL, y = "wait time in minutes") +
  theme(legend.position = "none",
        axis.ticks.x    = element_blank(),
        panel.grid      = element_blank()) +
  facet_wrap(~cafe, ncol = 1)
```

Now lets recreate our parameters from our simulated data with the following model. In line 3, we specify that the slope and intercept covary through a joint multivariate guassian distribution. In line 4 we specify the covariance structure.

$$
W_i \sim Normal(\mu_i, \sigma)
\\
\mu_i = \alpha_{cafe[i]} + \beta_{cafe[i]}A_i
\\
\begin{bmatrix} \alpha_{cafe} \\ \beta_{cafe} \\ \end{bmatrix} \sim MVNormal(\begin{bmatrix} \alpha \\ \beta \end{bmatrix}, S)
\\
S = \begin{bmatrix} \sigma_\alpha & 0 \\ 0 & \sigma_\beta\end{bmatrix} \ R \ \begin{bmatrix} \sigma_\alpha & 0 \\ 0 & \sigma_\beta\end{bmatrix}
\\
\alpha \sim Normal(5, 2)
\\
\beta \sim Normal(-1, .5)
\\
\sigma \sim Exponential(1)
\\
\sigma_{\alpha} \sim Exponential(1)
\\
\sigma_{\beta} \sim Exponential(1)
\\
R \sim LKJcorr(2)
$$

The final line of the model defines the prior for our covariances. Right now, we only have one slope and one intercept that covary: $ R = \begin{bmatrix} 1 & \rho \\ \rho & 1 \end{bmatrix}$. So we'd really only need to define a prior for $\rho$. When there is more than that, its useful to use the _LKJcorr distribution_.

> You can think of it as a regularizing prior for correlations. This distribution has a single parameter, η, that controls how skeptical the prior is of large correlations in the matrix. When we use LKJcorr(1), the prior is flat over all valid correlation matrices. When the value is greater than 1, such as the 2 we used above, then extreme correlations are less likely. 

```{r}
n_sim <- 1e5
set.seed(133)
etas <- c(1, 2, 4)

etas %>% 
  map_dfc(~ rethinking::rlkjcorr(n_sim, K = 2, eta = .x)[,1,2]) %>% 
  set_names(as.character(etas)) %>% 
  gather(Eta, draw) %>% 
  ggplot(aes(x = draw, fill = Eta)) +
    geom_density(colour = "transparent", alpha = I(1/3)) +
    theme(panel.grid = element_blank(),
          axis.ticks.y = element_blank(),
          axis.text.y = element_blank()) +
    xlab("Correlation") + ylab("Density") +
    ggtitle("lkjCorr distribution",
            "Higher etas specify more skepticism of covariance") +
    scale_x_continuous(breaks = c(-1, 1)) +
    scale_fill_manual(values = c("steelblue", "firebrick", "yellow"))
```

With that distribution in our arsenal, lets get modeling! In our regression formula, we specify that both the intercept and time-of-day covary with cafes. 

```{r}
get_prior(data = d, family = gaussian,
      wait ~ 1 + afternoon + (1 + afternoon | cafe))
```


```{r}
 b13.1 <- 
  brm(data = d, family = gaussian,
      wait ~ 1 + afternoon + (1 + afternoon | cafe),
      prior = c(prior(normal(0, 10), class = Intercept),
                prior(normal(0, 10), class = b),
                prior(cauchy(0, 2) , class = sd),
                prior(cauchy(0, 2) , class = sigma),
                prior(lkj(2), class = cor)),
      sample_prior = "yes", refresh = 0,
      iter = 5000, warmup = 2000, chains = 2, cores = 2)
summary(b13.1)
```

Lets examine the learned correlation between our slopes and intercepts
```{r}
post <- posterior_samples(b13.1)

post %>%
  ggplot(aes(x = cor_cafe__Intercept__afternoon)) +
  geom_density(aes(x = prior_cor_cafe),
               color = "transparent", fill = "#EEDA9D", alpha = 3/4) +
  geom_density(color = "transparent", fill = "#A65141", alpha = 9/10) +
  annotate("text", label = "posterior", 
           x = -0.2, y = 2.2, 
           color = "#A65141", family = "Courier") +
  annotate("text", label = "prior", 
           x = 0, y = 0.85, 
           color = "#EEDA9D", alpha = 2/3, family = "Courier") +
  scale_y_continuous(NULL, breaks = NULL) +
  scale_x_continuous(breaks = c(-1, 0, 1)) +
  xlab("correlation")
```

```{r}
# Extract group level coefficients like so:
# coefficients$groupterm[allrows,estimates_only,parameters 1 and 2]
partially_pooled_estimates <-
  coef(b13.1)$cafe[ , 1, 1:2] %>%
  as_tibble() %>%
  rename(morning   = Intercept) %>% 
  mutate(afternoon = morning + afternoon,
         cafe      = 1:n()) %>%
  select(cafe, everything()) 

# Compute unpooled estimates directly from data
un_pooled_estimates <-
  d %>%
  group_by(afternoon, cafe) %>% 
  summarise(mean = mean(wait)) %>%
  ungroup() %>%
  mutate(afternoon = ifelse(afternoon == 0, "morning", "afternoon")) %>%
  spread(key = afternoon, value = mean)

estimates <-
  bind_rows(partially_pooled_estimates, un_pooled_estimates) %>%
  mutate(pooled = rep(c("partially", "not"), each = n() / 2))
  
ggplot(data = estimates, aes(x = morning, y = afternoon)) +
  # Nesting `stat_ellipse()` within `mapply()` is a less redundant way to produce the 
  # ten-layered semitransparent ellipses we did with ten lines of `stat_ellipse()` 
  # functions in the previous plot
  mapply(function(level) {
    stat_ellipse(geom  = "polygon", type = "norm",
                 size  = 0, alpha = 1/15,
                 level = level)
    }, 
    # Enter the levels here
    level = c(seq(from = 1/10, to = 9/10, by = 1/10), .99)) +
  geom_line(aes(group = cafe), size = 1/4) +
  geom_point(aes(group = cafe, color = pooled)) +
  scale_color_manual("Pooled?",
                     values = c("steelblue", "firebrick")) +
  coord_cartesian(xlim = range(estimates$morning),
                  ylim = range(estimates$afternoon)) +
  theme(panel.background = element_rect(fill = "transparent", color = "black"),
        panel.grid = element_blank(),
        legend.position = c(0, 1),
        legend.justification = c(0, 1),
        legend.background = element_blank()) +
  labs(x = "morning wait (mins)",
       y = "afternoon wait (mins)")
```

# Example: Admission decisions and gender

Let’s revisit the infamous UCB admissions data.
```{r}
suppressPackageStartupMessages(library(rethinking))
data(UCBadmit)
d <- UCBadmit
detach(package:rethinking, unload = T)
library(brms)
rm(UCBadmit)

d <- 
  d %>%
  mutate(male    = ifelse(applicant.gender == "male", 1, 0),
         dept_id = rep(1:6, each = 2)) %>% 
  select(admit, applications, dept_id, male)

d %>% glimpse()
```

We'll model the probability of admission with partial pooling among departments.
$$
admit_i \sim Binomial(n_i, p_i)
\\
logit(p_i) = \alpha_{dept[i]} + \beta * male_i
\\
\alpha_{dept} \sim Normal(\alpha, \sigma)
\\
\alpha \sim Normal(0, 10)
\\
\beta \sim Normal(0, 1)
\\
\sigma \sim HalfCauchy(0, 2)
$$

Lets fit this with stan and BRMS
```{r}
b13.2 <-
  brm(data = d, family = binomial,
      admit | trials(applications) ~ 1 + male + (1 | dept_id),
      prior = c(prior(normal(0, 1.5), class = Intercept),
                prior(normal(0, 1.5), class = b),
                prior(cauchy(0, 2), class = sd)),
      refresh = 0, chains = 4, cores = 4,
      control = list(adapt_delta = .99))

summary(b13.2)
```


```{r}
post <- posterior_samples(b13.2) post$b_Intercept

post %>% 
  transmute(diff = inv_logit_scaled(b_Intercept + b_male) - inv_logit_scaled(b_Intercept)) %>% 
  gather() %>% 
  ggplot(aes(x = value, y = key)) +
    geom_halfeyeh() +
    ggtitle("This model implies negligble effect of being male")
```

To confirm, lets also vary the marginal effect of being male by department.

$$
admit_i \sim Binomial(n_i , p_i)
\\
logit(p_i) = \alpha_{deptid} + \beta_{deptid}male_i
\\
\begin{bmatrix} \alpha_{deptid} \\ \beta_{deptid} \end{bmatrix} \sim MVNormal(\begin{bmatrix} \alpha \\ \beta \end{bmatrix}, \textbf{S})
\\
S = \begin{bmatrix} \sigma_{\alpha} & 0 \\ 0 & \sigma_{\alpha} \end{bmatrix} \textbf{R} \begin{bmatrix} \sigma_{\alpha} & 0 \\ 0 & \sigma_{\alpha} \end{bmatrix}
\\
\alpha \sim Normal(0, 1.5)
\\
\beta \sim Normal(0, 1)
\\
(\sigma_{\alpha}, \sigma_{\beta}) \sim HalfCauchy(0, 2)
\\
\textbf{R} \sim LKJcorr(2)
$$

Lets fit this in STAN
```{r}
b13.3 <- 
  brm(data = d, family = binomial,
      admit | trials(applications) ~ 1 + male + (1 + male | dept_id),
      prior = c(prior(normal(0, 1.5), class = Intercept),
                prior(normal(0, 1), class = b),
                prior(cauchy(0, 2), class = sd),
                prior(lkj(2), class = cor)),
      chains = 4, cores = 4, refresh = 0)
summary(b13.3)
```

What we find below is that department intercepts are far more dispersed than gender. Being male's marginal effect is largely negligable.

```{r}
post <- posterior_samples(b13.3)
post %>% 
  select(starts_with("r_")) %>% 
  gather() %>% 
  mutate(value = inv_logit_scaled(
    value + post$b_Intercept)) %>% 
  group_by(key) %>% 
  median_hdi(.width = c(.5, .95)) %>% 
  mutate(sort_idx = ifelse(str_detect(key, ".*Intercept.*"), 0, 1)) %>% 
  ggplot(aes(y = fct_reorder(key, sort_idx), x = value)) +
    geom_vline(xintercept = .5, linetype = 2, alpha = I(1/3)) +
    geom_pointintervalh() +
    theme(panel.grid       = element_blank(),
          panel.background = element_rect(fill = "transparent", colour = "black"),
          text             = element_text(family="Courier")) +
    xlab("Probability scaled marginal posterior distributions") + ylab("") +
    ggtitle("Gender is not a big influence on admission rates",
            "Shown with 50% and 95% credible intervals about the median")

```

# Example: Chimps again!

Lets revisit the chimps data set again to show off a model with multiple clusters
```{r data load}
suppressPackageStartupMessages(library(rethinking))
data(chimpanzees)
d <- chimpanzees
detach("package:rethinking")
d <- d %>% 
  mutate(
    treatment = 1 + prosoc_left + 2*condition,
    treatment = as.integer(treatment)) %>% 
  select(
    actor, block, pulled_left, treatment) %>% 
  rename(
    block_id = block,
    actor_id = actor,
    treatment_id = treatment)
df <- d %>% 
  transmute(
    actor_id = as.factor(actor_id),
    block_id = as.factor(block_id),
    treatment_1 = as.integer(ifelse(treatment_id == 1, 1, 0)),
    treatment_2 = as.integer(ifelse(treatment_id == 2, 1, 0)),
    treatment_3 = as.integer(ifelse(treatment_id == 3, 1, 0)),
    treatment_4 = as.integer(ifelse(treatment_id == 4, 1, 0)),
    pulled_left = as.integer(pulled_left))
df
```

Our model will contain:

1. The average log odds of each treatment (4 parameters)
2. An effect for each actor in eaach treatment (7 x 4 = 28 parameters)
3. An effect for each block in each treatment (6 x 4 = 24 parameters)

We'll utilize partial pooling for actors and blocks to get sensible estimates

$$
\begin{align}
L_i &\sim Binomial(1, p_i)
\\
logit(p_i) &= \gamma_{TID[i]} + \alpha_{ACTOR[i], TID[i]} + \beta_{BLOCK[i],TID[i]}
\\
\begin{bmatrix} \alpha_{j,1} \\\alpha_{j,2} \\\alpha_{j,3} \\\alpha_{j,4} \end{bmatrix} & \sim MVNormal(
    \begin{bmatrix} 0 \\ 0 \\ 0 \\ 0 \end{bmatrix}
  , S_{ACTOR})
\\
\begin{bmatrix} \beta_{j,1} \\\beta_{j,2} \\\beta_{j,3} \\\beta_{j,4} \end{bmatrix} & \sim MVNormal(
    \begin{bmatrix} 0 \\ 0 \\ 0 \\ 0 \end{bmatrix}
  , S_{BLOCK})
\\
S_{ACTOR} &\sim 
  \begin{pmatrix} 
    \sigma_{\alpha_{TRT[1]}} & 0 & 0 & 0 \\ 
    0 & \sigma_{\alpha_{TRT[1]}} & 0 & 0 \\
    0 & 0 & \sigma_{\alpha_{TRT[1]}} & 0 \\
    0 & 0 & 0 & \sigma_{\alpha_{TRT[1]}} \\
  \end{pmatrix} R_{ACTOR} \begin{pmatrix} 
    \sigma_{\alpha_{TRT[1]}} & 0 & 0 & 0 \\ 
    0 & \sigma_{\alpha_{TRT[1]}} & 0 & 0 \\
    0 & 0 & \sigma_{\alpha_{TRT[1]}} & 0 \\
    0 & 0 & 0 & \sigma_{\alpha_{TRT[1]}} \\
  \end{pmatrix}
\\
S_{BLOCK} &\sim
   \begin{pmatrix} 
    \sigma_{\beta_{TRT[1]}} & 0 & 0 & 0 \\ 
    0 & \sigma_{\beta_{TRT[1]}} & 0 & 0 \\
    0 & 0 & \sigma_{\beta_{TRT[1]}} & 0 \\
    0 & 0 & 0 & \sigma_{\beta_{TRT[1]}} \\
  \end{pmatrix} R_{BLOCK} \begin{pmatrix} 
    \sigma_{\beta_{TRT[1]}} & 0 & 0 & 0 \\ 
    0 & \sigma_{\beta_{TRT[1]}} & 0 & 0 \\
    0 & 0 & \sigma_{\beta_{TRT[1]}} & 0 \\
    0 & 0 & 0 & \sigma_{\beta_{TRT[1]}} \\
  \end{pmatrix}
\\
\gamma &\sim normal(0, 1)
\\
R_{ACTOR},\ R_{BLOCK} &\sim LKJ(4)
\\
\sigma_{\alpha},\ \sigma_\beta &\sim exponential(1)
  
\end{align}
$$

In the specification above actors and blocks come from two different statistical populations. Within each, the four features of each block/actor (treatment_id) are related through a covariance matrix.

> This is essentially an interaction model that allows for effects of each treatment to vary by each actor and each block.

Now then, lets run this in STAN.
```{r}
# Set up model specification
specification <- bf(pulled_left ~ 0 + treatment_1 + treatment_2 + treatment_3 + treatment_4 + 
                      (treatment_1 + treatment_2 + treatment_3 + treatment_4 | actor_id + block_id),
                    family = bernoulli)

fit_prior <- get_prior(specification, df)
fit_prior[1:5,]$prior <- "normal(0, 1)"
fit_prior[6:8,]$prior <- "lkj(4)"
fit_prior[9:21,]$prior <- "exponential(1)"

b1 <- brm(specification, data = df, family = bernoulli, 
    prior = fit_prior, cores = 4, refresh = 0)
summary(b1)
```

```{r}
spread_draws(b1, `b_.*`, regex = TRUE) %>% 
  gather(treatment, value, b_treatment_1, b_treatment_2, b_treatment_3, b_treatment_4) %>% 
  mutate(value = inv_logit_scaled(value)) %>% 
  select(treatment, value) %>% 
  group_by(treatment) %>% 
  ggplot(aes(y = treatment, x = value)) +
    geom_vline(xintercept = .5, linetype = 2) +
    geom_halfeyeh(fill = alpha("steelblue", .5)) +
    ylab("") + xlab("Posterior average treatment effect") +
    theme(panel.grid = element_blank(),
          text = element_text(family = "Courier"),
          axis.ticks = element_blank())
```

```{r}
spread_draws(b1, r_actor_id[actor, treatment]) %>% 
  spread(treatment, r_actor_id) %>% 
  ungroup() %>% 
  transmute(
    actor = as.factor(actor),
    treatment_1 = inv_logit_scaled(treatment_1 + Intercept),
    treatment_2 = inv_logit_scaled(treatment_2 + Intercept),
    treatment_3 = inv_logit_scaled(treatment_3 + Intercept),
    treatment_4 = inv_logit_scaled(treatment_4 + Intercept)) %>% 
  gather(treatment, value, treatment_1, treatment_2, treatment_3, treatment_4) %>% 
  ggplot(aes(y = treatment, x = value)) +
    geom_halfeyeh() + ylab("") +
    facet_wrap(~ actor, labeller = label_both) +
    ggtitle("Most variation is across actors")
```

```{r}
spread_draws(b1, r_block_id[block, treatment]) %>% 
  spread(treatment, r_block_id) %>% 
  ungroup() %>% 
  transmute(
    block = as.factor(block),
    treatment_1 = inv_logit_scaled(treatment_1 + Intercept),
    treatment_2 = inv_logit_scaled(treatment_2 + Intercept),
    treatment_3 = inv_logit_scaled(treatment_3 + Intercept),
    treatment_4 = inv_logit_scaled(treatment_4 + Intercept)) %>% 
   gather(treatment, value, treatment_1, treatment_2, treatment_3, treatment_4) %>% 
   ggplot(aes(y = treatment, x = value)) +
     geom_halfeyeh() + ylab("") +
     facet_wrap(~ block, labeller = label_both) +
     ggtitle("Blocks don't matter much")
```

> Our interpretation of this experiment has not changed. These chimpanzees simply did not behave in any consistently different way in the partner treatments. 

