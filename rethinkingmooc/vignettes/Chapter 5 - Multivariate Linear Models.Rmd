---
title: "Chapter 5 - Multivariate Linear Models"
output: html_notebook
---

```{r setup}
knitr::opts_chunk$set(fig.height = 3.5, fig.width = 5)
```


# Introduction

In large data sets, every pair of variables has a statistically discernible non-zero correlation. Most of these relationships are non-causal. Multivariate regression can help us examine causal relatinoships by:

1. Statistical control for confounders. A counfound is a variable that may be correlated with another variable of interest.
2. Multiple Cuasation. A phenomonen may really arise from multiple causes.
3. Interactions. Even when variables are completely uncorrelated, the importance of each may still depend upon the other.

# Spurious association
In statistics, a spurious relationship or spurious correlation is a mathematical relationship in which two or more events or variables are not causally related to each other, yet it may be wrongly inferred that they are, due to either coincidence or the presence of a certain third, unseen factor referred to as a `confounding factor`.

In this example, we'll examine `divorce rate`, `median age at marriage`, and `marriage rate` in a data set where each observation is a state. 
```{r}
library(rethinking)
library(tidyverse)
devtools::load_all()
rstan_options(auto_write = TRUE)

data(WaffleDivorce)
d <- WaffleDivorce
d %>% 
  skimr::skim()
```



```{r fig.height=3.5, fig.height=6}
# Standardize the predictor `median age at marriage`
d$MedianAgeMarriage.s <- (d$MedianAgeMarriage - mean(d$MedianAgeMarriage)) / sd(d$MedianAgeMarriage)

# Fit divorce rate 
m5.1 <- rethinking::map(
  alist(
    Divorce ~ dnorm(mu, sigma),
    mu <- a + bA * MedianAgeMarriage.s,
    a ~ dnorm(10, 10),
    bA ~ dnorm(0, 1),
    sigma ~ dunif(0, 10)), 
  data = d)

# Standardize the predictor `Marriage`
d$Marriage.s <- (d$Marriage - mean(d$Marriage)) / sd(d$Marriage)

# Fit it's relationship with divorce rate
m5.2 <- rethinking::map(
  alist(
    Divorce ~ dnorm(mu, sigma),
    mu <- a + bR * Marriage.s,
    a ~ dnorm(10, 10),
    bR ~ dnorm(0, 1),
    sigma ~ dunif(0, 10)),
  data = d)

par(mfrow=c(1,2)) 
m5.1 %>% 
  precis %>% 
  plot(main="Median age at marriage")
m5.2 %>% 
  precis %>% 
  plot(main = "Marriage rate")
```

The regression above shows that each additional standard deviation of delay in marriage (1.24 years) predicts a decrease of about one divorce per thousand adults, withh an 89% interval from -1.4 to -0.7. It also shows an increase of .6 divorces for every additional standard deviation of marriage rate (3.8). However, merely comparing paramter means between different bivariate regressions tells us very little. Both predictors could provide independent value, be redundant, or eliminate the value of the other. 

What we should do is model them together in a multivariate model. We want to know, `What is the predictive value of a variable once I already know all of the other predictor variables.`

```{r}
m5.3 <- rethinking::map(
  alist(
    Divorce ~ dnorm(mu, sigma),
    mu <- a + bR*Marriage.s + bA*MedianAgeMarriage.s,
    a ~ dnorm(10, 10),
    bR ~ dnorm(0, 1),
    bA ~ dnorm(0, 1),
    sigma ~ dunif(0, 10)),
  data = d)

m5.3 %>% 
  precis %>% 
  plot
```

The posterior mean for marginal effect of state's `marraige rate` on `divorce rate`, `bR`, is now close to zero. Once we know median age at marriage for a state, there is little or no additional predictive power in also knowing the rate of marriage in that state.

# Plotting multivariate posteriors

Multivariate regressions can get complicated. To interrogate them better, we can use plots to our advantage. There are some plot types to interpret models.

1. _Predictor residual plots_: Show the outcome against residual predictor values.
2. _Counterfactual plots_: Show implied predictions for made up data.
3. _Posterior prediction plots_: Model based predictions against raw data.

## Predictor residual plots

A predictor variable residual is the average prediction error when we use all of the other predictor variables to model a predictor of interest. This allows us to visualize the bivariate relationship between the predictor and the outcome while "controlling" for all of the other predictor variables.

```{r fig.width=4.5}
# Model marriage as a function of Median Marriage Age
m5.4 <- rethinking::map(
  alist(
    Marriage.s ~ dnorm(mu, sigma),
    mu <- a + b*MedianAgeMarriage.s,
    a ~ dnorm(0, 10),
    b ~ dnorm(0, 1),
    sigma ~ dunif(0, 10)),
  data = d)

# Compute residuals
mu.hat <- coef(m5.4)['a'] + coef(m5.4)['b']*d$MedianAgeMarriage.s
m.resid <- d$Marriage.s - mu.hat

# Generate predictor residual plot
data_frame(
  divorce_rate = d$MedianAgeMarriage.s,
  marriage_rate = m.resid
) %>% 
  ggplot(aes(marriage_rate, divorce_rate)) +
    #geom_point(shape=1, size=3) +
    geom_point(colour="steelblue", size=3, alpha=I(1/3)) +
    geom_smooth(method=lm, formula = y ~ poly(x, 1), colour="black") +
    geom_vline(xintercept=0, linetype=2) +
    theme_bw() + theme_empty() +
    ggtitle("Predictor residual plot") +
    xlab("Marriage Rate Residuals") + ylab("Divorce Rate") + 
    annotate("text", x=-.21, y=3, label = "Slower") +
    annotate("text", x=.2, y=3, label = "Faster")

```

Above, we plot the residuals from using median age at marriage to predict marriage rate against our outcome of interest. This plot displays the linear relationship between `divorce` and `marriage rates`, having statistically "controlled" for `median age of marriage`. Recall that multiple linear regression models measure the remaining association of each predictor with the outcome. By computing the predictor residual plots you perform those calculations yourself. 

What does it mean for residuals to be explained well by another variables?

## Counterfactual plot

In counterfactual plots, we hold all predictor variables to their mean value except for one in particular. We plot this one varying predictor against our outcome of interest. We can generate this for each predictor variable independently.

```{r fig.width=4.5}
# Prepare counterfactual data
age_avg <- mean(d$MedianAgeMarriage.s)
new_marriage_rate <- seq(from = -3, to = 3, length.out = 30)
new_data <- data_frame(
  Marriage.s = new_marriage_rate,
  MedianAgeMarriage.s = age_avg)

# Calculate mean divorce (mu) and percentile interval
mu <- link(m5.3, data = new_data)
mu_mean <- apply(mu, 2, mean)
mu_pi <- apply(mu, 2, PI)

# Simulate counterfactual divorce outcomes and percentie interval
divrate_sim <- sim(m5.3, data = new_data, n = 1e4)
divrate_pi <- apply(divrate_sim, 2, PI)

# counterfactual plot
data_frame(
  new_marriage_rate,
  mean_divorce_rate = mu_mean,
  mean_divorce_rate_min = mu_pi[1,],
  mean_divorce_rate_max = mu_pi[2,],
  divorce_rate_min = divrate_pi[1,],
  divorce_rate_max = divrate_pi[2,]) %>% 
  
  ggplot(aes(x=new_marriage_rate, y=mean_divorce_rate)) +
    geom_line() +
    geom_ribbon(aes(ymin = mean_divorce_rate_min, ymax = mean_divorce_rate_max), alpha = I(1/5)) +
    geom_ribbon(aes(ymin = divorce_rate_min, ymax = divorce_rate_max), alpha = I(1/5)) +
    coord_cartesian(xlim=c(-2, 3), ylim=c(6, 13)) +
    ylab("Divorce") + xlab("Marriage Rate") +
    ggtitle("Counterfactual plot", "MedianAgeMarriage.s = 0") +
    theme_empty() + geom_vline(xintercept = 0, linetype=2) +
    annotate("text", -.25, 13, label="Slower") +
    annotate("text", .3, 13, label="Faster")
```

## Posterior prediction plots

This is just a plot of predicted vs actual.

1. Did the model fit correctly?
2. How does the model fail?

```{r}
# Run link on training data
mu <- link(m5.3)

# Summarize the samples
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI)

# Simulate observations
divorce.sim <- sim(m5.3, n=1e4)
divorce.PI <- apply(divorce.sim, 2, PI)

# Plot predicted against actual
data_frame(x = d$Divorce,
           y = mu.mean,
           ymin=mu.PI[1,],
           ymax=mu.PI[2,]) %>% 
    ggplot(aes(x, y)) + 
      geom_point(shape = 1, size = 2) +
      geom_linerange(aes(ymin = ymin, ymax = ymax)) +
      geom_abline(intercept = 0, slope = 1, linetype=2) + 
      xlab("Observed Divorce") + ylab("Predictived Divorce") +
      ggtitle("Posterior prediction plot")
```

## Revisiting spurious predictors
Lets create an explicit example of spurious prediction. Below, `x_real` is the actual causal parameter of interest but it is correlated with a spurious variable, `x_spur` but it is correlated with both y and x_spur
```{r}
N <- 100
df <- data_frame(x_real = rnorm(N),
                 y = rnorm(N, x_real),
                 x_spur = rnorm(N, x_real))
ggplot(df, aes(y=y)) +
  geom_smooth(aes(x=x_spur), method=lm, colour="black") +
  xlab("A spurious variable") + ggtitle("An example of spurious association") +
  theme_empty()

```

# Masked relationship

Bivariate relationships can be misleading when two predictor variables are correlated with each other but one variable is positively correlated with the outcome and the other is negatively correlated with it. Below, we will simulate data in which two meaningful predictors act to mask one another. 

```{r}
# Required for pair plot
library(GGally)

# number of cases
N <- 100
# correlation b/w x_pos and x_neg
rho <- .7  

df = data_frame(x_pos = rnorm(N),
                x_neg = rnorm(N, rho*x_pos, 
                              sqrt(1-rho^2)),
                y = rnorm(N , x_pos - x_neg))

GGally::ggpairs(df)
```

It looks like Y has nothing to do with either x_pos or x_neg, however, note that Y is a function of both x_pos and x_neg. A multivariate regression would pick up on this exactly.
```{r}
fit_fixed <- rethinking::map(
  alist(
    y ~ dnorm(mu, sigma),
    mu <- a + b*x_pos + c*x_neg,
    a ~ dnorm(0, 10),
    b ~ dnorm(0, 10),
    c ~ dnorm(0, 10),
    sigma ~ dunif(0, 10)),
  data = as.data.frame(df))

plot(precis(fit_fixed))
```

Simple bivariate regressions would miss this entirely.
```{r}
fit_masked <- rethinking::map(
  alist(
    y ~ dnorm(mu, sigma),
    mu <- a + b*x_pos,
    a ~ dnorm(0, 10),
    b ~ dnorm(0, 10),
    sigma ~ dunif(0, 10)),
  data = as.data.frame(df))

plot(precis(fit_masked))
```

# When adding variables hurts

There are a few reasons why you don't just want to chuck every variable you have in to your data frame.

1. *Multicollinearity*: Very strong correlation between two or more predictor variables. Multiple linear regression answers the question, "`What is the value of knowing each predictor, after already knowing all of the other predictors?`". In cases where variables are correlated, a single variables estimate will be deflated as its effect on the outcome is already being explained by another variable.
2. *Post treatment bias*: 
3. *Overfitting*:

## Multicollinearity

Multiple linear regression answers the question, "`What is the value of knowing each predictor, after already knowing all of the other predictors?`". In cases where variables are correlated, a single variables estimate will be deflated as its effect on the outcome is already being explained by another variable. As an example, below we regress height as a function of left and right leg length.
```{r}
N <- 100

df <- data_frame(
  # Each individual's height
  height = rnorm(N, 10, 2),
  # Leg as proportion of height
  leg_prop = runif(N, .4, .5),
  # left leg as proportion + error
  leg_left = leg_prop * height + rnorm(N, 0, .02),
  # righ tleg as proportion + error
  leg_right = leg_prop * height + rnorm(N, 0, .02)) %>% 
  select(height, leg_left, leg_right)

m5.8 <- rethinking::map(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + bl*leg_left + br*leg_right,
    a ~ dnorm(10, 100),
    bl ~ dnorm(2, 10),
    br ~ dnorm(2, 10),
    sigma ~ dunif(0, 10)
  ), data=as.data.frame(df))
plot(precis(m5.8))
```

When we ask the question, `What is the value of knowing each leg's length, after already knowing the other leg's length?`, the results in the plot above should be intuitive.

Recall that the posterior distribution samples every possible combination of parameters according to their plausibilities conditional on the model specification, the data, and the prior. Sampling from the posterior and plotting left and right leg length against each other shows how correlated they are.

```{r}
post <- extract.samples(m5.8, n=1e4)
ggplot(post, aes(bl, br)) + 
  geom_bin2d(bins=40) + theme_empty() +
  xlab("left leg") + ylab("right leg") +
  ggtitle("Posterior sample", "These two predictors are highly correlated")
```

In summary, including two predictor variables that are strongly correlated in your model can lead to confusion. If all you're interested in is prediction though, you have nothing to worry about. 

## Post treatment bias
Post treatment bias occurs when you include variables that are consequences of other variables. It has essentially the same effect as multicollinearity in that you'll be able to say less about the original variable if you know about the consequenced variable already.

# Categorical variables
Categorical variables need to be encoded properly for good interpretation. For example, if your regression specification looks like this: $height_i = \beta_0 + \beta_1 isMale$ you can interpret $\beta_0$ as the average height of a female. Therefore, $\beta_1$ is the average difference in heights between males and females. Its also important to note that these two paramters are correlated, so you should take care to examine their intervals by relying on the posterior.

To include k categories in a linear model, you require k − 1 dummy variables. Each dummy variable indicates, with the value 1, a unique category. The category with no dummy variable assigned to it ends up again as the “intercept” category. Alternatively, you can create "intercept" terms for every single possible category value.
```{r eval=FALSE}
data(milk)
d <- milk

# Construct an index variable (picklist)
d$clade_id <- coerce_index(d$clade)

m5.16_alt <- rethinking::map(
  alist(
    kcal.per.g ~ dnorm( mu , sigma ) ,
    mu <- a[clade_id] ,
    a[clade_id] ~ dnorm( 0.6 , 10 ) ,
    sigma ~ dunif( 0 , 10 )
), data=d)
precis( m5.16_alt , depth=2 )
```


# Summary
This chapter introduced multiple regression, a way of constructing descriptive models for how the mean of a measurement is associated with more than one predictor variable. The defining question of multiple regression is: What is the value of knowing each predictor, once we already know the other predictors? Implicit in this question are: 

1. A focus on the value of the predictors for description of the sample, instead of forecasting a future sample
2. The assumption that the value of each predictor does not depend upon the values of the other predictors. In the next two chapters, we confront these two issues.


# Homework

## Medium

### 5M2
Invent yourown example of a masked relationship. An outcome variable should be correlated with both predictor variables, but in opposite directions. And the two predictor variables should be correlated with one another.

```{r}
N <- 1e4

data_frame(
  x1 = rnorm(N),
  x2 = rnorm(N, x1 + rnorm(N)),
  outcome = x2 - x1 + rnorm(N)) ->
df

ggpairs(df)
```


### 5M4
In the divorce data, States with high numbers of Mormons (members of The Church of Jesus ChristofLatter-daySaints, LDS) have much lower divorce rates than the regression models expected. Find a list of LDS population by State and use those numbers as a predictor variable, predicting divorce rate using marriage rate, median age at marriage, and percent LDS population (possibly standardized). You may want to consider transformations of the raw percent LDS variable.

First lets get LDS percentages by scraping the web and joining it back to our data frame
```{r}
library(rvest)

data(WaffleDivorce)

# Scrape internet
read_html("https://www.worldatlas.com/articles/mormon-population-by-state.html") %>%
  html_nodes(xpath='//*[@id="artReg-table"]/table') %>%
  html_table(trim=TRUE) %>%
  bind_rows %>% as_tibble %>%
  
  # Clean columnames
  magrittr::set_colnames(c("Rank", "State", "Mormon_Population", "Total_Population", "Percentage_Mormon")) %>% 
  
  # Join data sets
  inner_join(as_data_frame(WaffleDivorce), by=c("State" = "Location")) %>% 
  
  # Center and scale predictors
  transmute(
    Location = State,
    Percentage_Mormon = scale(as.numeric(str_remove(Percentage_Mormon, "%"))),
    Median_Age = scale(MedianAgeMarriage),
    Marriage_Rate = scale(Marriage),
    Divorce_Rate = Divorce) ->
df_lds

df_lds
```

```{r}
# Bootstrap the dataset 1000 times
rsample::bootstraps(df_lds, times = 1e4) %>% 
  pull(splits) %>% 
  
  # Run regression on each sample
  purrr::map(~ lm(Divorce_Rate ~ Marriage_Rate + Median_Age + Percentage_Mormon, data = .x)$coefficients) %>% 
  
  # Pull out coefficients
  reduce(bind_rows) %>% 
  gather("Parameter", "Value") %>% 
  group_by(Parameter) %>%
  # Summarize the bootstrapped sample
  mutate(
    ymin = quantile(Value, .05),
    ymax = quantile(Value, .95),
    Value = quantile(Value, .5)) %>% 
  unique %>% 
  
  # Plot coefficients
  ggplot(aes(Parameter, Value)) +
  geom_errorbar(aes(ymin=ymin, ymax=ymax)) +
  geom_hline(yintercept=0) +
  coord_flip() + theme_empty() +
  ggtitle("Coefficient Measurments", "Knowing a states Mormon percentage makes it's marriage rate unimportant.")
```

## Hard
```{r}
data(foxes)
df_foxes <- as_data_frame(foxes)
df_foxes
```

### 5H1
```Fit two bivariate Gaussian regressions, using map. Plot the results of these regressions, displaying the MAP regression line and the 95% interval of the mean. Is either variable important for predicting fox body weight?```

```(1) body weight as a linear function of territory size (area)```
```{r}
# Fit model
m5.5h1.1 <- rethinking::map(
  alist(
    weight ~ dnorm( mu , sigma ),
    mu <- Intercept +
        b_area*area,
    Intercept ~ dnorm(0,10),
    b_area ~ dnorm(0,10),
    sigma ~ dunif(0, 10)
), data = as.data.frame(df_foxes))

# Counterfactual x-axis
area_axis = seq(from=1, to=6, length.out=100)
# Sample from the posterior
posterior_samples <- extract.samples(m5.5h1.1)
# Generate link function
mu_link <- function(area) {
  posterior_samples$Intercept + (posterior_samples$b_area * area)
}
# Generate predictions for each sample
mu <- purrr::map(area_axis, mu_link)

data_frame(
  area = area_axis,
  mu_mean = map_dbl(mu, mean),
  mu_lower = map_dbl(mu, ~HPDI(.x, .95)[1]),
  mu_upper = map_dbl(mu, ~HPDI(.x, .95)[2])
) %>% 
  ggplot(aes(area, mu_mean)) + 
  geom_line() +
  geom_ribbon(aes(ymin = mu_lower, ymax = mu_upper), alpha = I(1/4)) +
  xlab("Area") + ylab("Weight") + theme_empty() +
  coord_cartesian(ylim = c(2, 7))
```

```(2) body weight as a linear function of groupsize. ```
```{r}
# Fit model
m5.5h1.2 <- rethinking::map(
  alist(
    weight ~ dnorm( mu , sigma ),
    mu <- Intercept +
        b_groupsize*groupsize,
    Intercept ~ dnorm(0,10),
    b_groupsize ~ dnorm(0,10),
    sigma ~ dunif(0, 10)
), data = as.data.frame(df_foxes))

# Counterfactual x-axis
groupsize_axis = seq(from=2, to=8, length.out=100)
# Sample from the posterior
posterior_samples <- extract.samples(m5.5h1.2)
# Generate link function
mu_link <- function(groupsize) {
  posterior_samples$Intercept + (posterior_samples$b_groupsize * groupsize)
}
# Generate predictions for each sample
mu <- purrr::map(groupsize_axis, mu_link)

data_frame(
  groupsize = groupsize_axis,
  # Summarize the set of predictions 
  # for each sample in the posterior
  mu_mean = map_dbl(mu, mean),
  mu_lower = map_dbl(mu, ~HPDI(.x, .95)[1]),
  mu_upper = map_dbl(mu, ~HPDI(.x, .95)[2])
) %>% 
  ggplot(aes(groupsize, mu_mean)) + 
  geom_line() +
  geom_ribbon(aes(ymin = mu_lower, ymax = mu_upper), alpha = I(1/4)) +
  xlab("Group Size") + ylab("Weight") + theme_empty()
```

Neither variable appears to be very important.

### 5H2
```Now fit a multiple linear regression with weight as the outcome and both area and groupsize as predictor variables.```

```{r}
rethinking::map(
  alist(
    weight ~ dnorm( mu , sigma ),
    mu <- Intercept +
        b_groupsize*groupsize +
        b_area*area,
    Intercept ~ dnorm(0,10),
    b_groupsize ~ dnorm(0,10),
    b_area ~ dnorm(0,10),
    sigma ~ dunif(0, 10)
    ), data = as.data.frame(df_foxes)
) -> m5.5h2

plot(precis(m5.5h2))
```

```Plot the predictions of the model for each predictor, holding the other predictor constant at its mean.```

First we interrogate the posterior.
```{r}
posterior_samples <- extract.samples(m5.5h2)

mu_link <- function(groupsize, area) {
  posterior_samples$Intercept +
    (posterior_samples$b_groupsize * groupsize) + 
    (posterior_samples$b_area * area)
}

groupsize_axis <- seq(from = 2, to = 8, length.out = 100)
area_mean <- mean(df_foxes$area)
mu_groupsize <- purrr::map(groupsize_axis,
                           ~ mu_link(.x, area_mean))

area_axis <- seq(from = 1, to = 6, length.out = 100)
groupsize_mean = mean(df_foxes$groupsize)
mu_area <- purrr::map(area_axis,
                      ~ mu_link(groupsize_mean, .x))
```

Counterfactual plot for groupsize
```{r}
data_frame(
  groupsize = groupsize_axis,
  weight = map_dbl(mu_groupsize, mean),
  weight_lower = map_dbl(mu_groupsize, ~HPDI(.x, .95)[1]),
  weight_upper = map_dbl(mu_groupsize, ~HPDI(.x, .95)[2])) %>% 
  ggplot(aes(groupsize, weight)) +
    geom_line() +
    geom_ribbon(aes(ymin = weight_lower, ymax = weight_upper), alpha = I(1/5)) +
    ggtitle("Counterfactual plot", glue::glue("Area = {round(area_mean, 2)}")) +
    theme_empty()
```

Counterfactual plot for area
```{r}
data_frame(
  area = area_axis,
  weight = map_dbl(mu_area, mean),
  weight_lower = map_dbl(mu_area, ~HPDI(.x, .95)[1]),
  weight_upper = map_dbl(mu_area, ~HPDI(.x, .95)[2])) %>% 
  ggplot(aes(area, weight)) +
    geom_line() +
    geom_ribbon(aes(ymin = weight_lower, ymax = weight_upper), alpha = I(1/5)) +
    ggtitle("Counterfactual plot", glue::glue("Group size = {round(groupsize_mean, 2)}")) +
    theme_empty()
```

```What does this model say about the importance of each variable? Why do you get different results than you got in the exercise just above?```

This is a masking relationship.

### 5H3. 
`Finally, consider the avgfood variable. Fit two more multiple regressions: 

(1) body weight as an additive function of avgfood and groupsize`
```{r}
m5.mh3.1 <- rethinking::map(
  alist(
    weight ~ dnorm( mu , sigma ),
    mu <- Intercept +
        b_avgfood*avgfood +
        b_groupsize*groupsize,
    Intercept ~ dnorm(0,10),
    b_avgfood ~ dnorm(0,10),
    b_groupsize ~ dnorm(0,10),
    sigma ~ dunif(0,10)
    ), data = as.data.frame(df_foxes)
  )

plot(precis(m5.mh3.1))
```


`(2) body weight as an additive function of all three variables, avgfood and groupsize and area.`
```{r}
m5.mh3.2 <- rethinking::map(
  alist(
    weight ~ dnorm( mu , sigma ),
    mu <- Intercept +
        b_avgfood*avgfood +
        b_groupsize*groupsize +
        b_area*area,
    Intercept ~ dnorm(0,10),
    b_avgfood ~ dnorm(0,10),
    b_groupsize ~ dnorm(0,10),
    b_area ~ dnorm(0,10),
    sigma ~ dunif(0,10)
    ), data = as.data.frame(df_foxes)
  )

plot(precis(m5.mh3.2))
```

`Compare the results of these models to the previous models you’ve fit, in the first two exercises. 

(a) Is avgfood or area a better predictor of body weight? If you had to choose one or the other to include in a model, which would it be? Support your assessment with any tables or plots you choose. `
```{r}
df_foxes %>% 
  select(avgfood, groupsize, area, weight) %>% 
  GGally::ggscatmat()
```


`(b) When both avgfood or area are in the same model, their effects are reduced (closer to zero) and their standard errors are larger than when they are included in separate models. Can you explain this result?`

This is because they are highly correlated.

# Recoded

```{r}
library(fiftystater)

d %>% 
  # first we'll standardize the three variables to put them all on the same scale
  mutate(Divorce_z = (Divorce - mean(Divorce)) / sd(Divorce),
         MedianAgeMarriage_z = (MedianAgeMarriage - mean(MedianAgeMarriage)) / sd(MedianAgeMarriage),
         Marriage_z = (Marriage - mean(Marriage)) / sd(Marriage),
         # need to make the state names lowercase to match with the map data
         Location = str_to_lower(Location)) %>% 
  # here we select the relevant variables and put them in the long format to facet with `facet_wrap()`
  select(Divorce_z:Marriage_z, Location) %>% 
  gather(key, value, -Location) %>% 
  
  ggplot(aes(map_id = Location)) +
  geom_map(aes(fill = value), map = fifty_states, 
           color = "firebrick", size = 1/15) +
  expand_limits(x = fifty_states$long, y = fifty_states$lat) +
  scale_x_continuous(NULL, breaks = NULL) +
  scale_y_continuous(NULL, breaks = NULL) +
  scale_fill_gradient(low = "#f8eaea", high = "firebrick4") +
  coord_map() +
  theme_bw() +
  theme(panel.grid       = element_blank(),
        legend.position  = "none",
        strip.background = element_rect(fill = "transparent", color = "transparent")) +
  facet_wrap(~key)

```

```{r fig.height=5, fig.width=6}
library(brms)
data(WaffleDivorce)
d <- WaffleDivorce

d2 <- d %>%
  mutate(MedianAgeMarriage_s = (MedianAgeMarriage - mean(MedianAgeMarriage)) / sd(MedianAgeMarriage),
         Marriage_s = (Marriage - mean(Marriage)) / sd(Marriage))

b5.3 <- 
  brm(data = d2, family = gaussian,
      Divorce ~ 1 + Marriage_s + MedianAgeMarriage_s,
      prior = c(prior(normal(10, 10), class = Intercept),
                prior(normal(0, 1), class = b),
                prior(uniform(0, 10), class = sigma)),
      iter = 2000, warmup = 500, chains = 4, cores = 4)

plot(b5.3)
```

## Coefficient Plots
```{r fig.height=1.5, fig.width=5.5, message=FALSE, warning=FALSE}
library(tidybayes)

tidy_coef_plot <- function(post, title="Coefficient Plot") {
  post %>% 
  select(-lp__) %>% 
  gather() %>% 
  # use `reorder()` to arrange the coefficients
  ggplot(aes(x = value, y = reorder(key, value))) +
  geom_vline(xintercept = 0, color = "firebrick4", alpha = 1/10) +
  stat_pointintervalh(point_interval = mode_hdi, .width = .95, 
                      size = 3/4, color = "firebrick4") +
  labs(title = title,
       x = NULL, y = NULL) +
  theme_bw() +
  theme(panel.grid   = element_blank(),
        panel.grid.major.y = element_line(color = alpha("firebrick4", 1/4), linetype = 3),
        axis.text.y  = element_text(hjust = 0),
        axis.ticks.y = element_blank())
}

tidy_coef_plot(posterior_samples(b5.3))
```

## Categorical variables

### Binary Variables
```{r message=FALSE, warning=FALSE}
library(rethinking)
data(Howell1)
d <- Howell1

rm(Howell1)
detach(package:rethinking, unload = T)
library(brms)

d %>% 
  glimpse
```


```{r}
b5.15 <- brm(data = d, family = gaussian,
             height ~ 1 + male,
             prior = c(prior(normal(178, 100), class = Intercept),
                       prior(normal(0, 10), class = b),
                       prior(cauchy(0, 2), class = sigma)),
             iter = 2000, warmup = 500, chains = 4, cores = 4)
plot(b5.15)
```

```{r}
post <- posterior_samples(b5.15)

post %>%
  transmute(male_height = b_Intercept + b_male) %>% 
  mean_qi(.width = .89)
```

```{r fig.height=1, fig.width=1.5}
fitted(b5.15,
       newdata = nd,
       summary = F) %>% 
  as_tibble() %>% 
  
  ggplot(aes(x = V1, y = 0)) +
    geom_halfeyeh(fill = "firebrick4", 
                point_interval = median_qi, .width = .95) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Model-implied male heights",
       x = expression(alpha + beta["male"])) +
  theme_bw() + theme(panel.grid = element_blank())
```

```{r}
d2 <- d %>% 
  mutate(female = male*-1 + 1)
b5.15b <- brm(data = d2, family = gaussian,
              height ~ 0 + male + female,
              prior = c(prior(normal(178, 100), class = b),
                        prior(cauchy(0, 2), class = sigma)),
              iter = 2000, warmup = 500, chains = 4, cores = 4)
plot(b5.15b)
```



```{r}
posterior_samples(b5.15b) %>% 
  transmute(dif = b_male - b_female) %>% 
  
  ggplot(aes(x = dif, y = 0)) +
    geom_halfeyeh(fill = "firebrick4", 
                point_interval = median_qi, .width = .95) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Model-implied difference score",
       x = expression(alpha["male"] - alpha["female"])) +
  theme_bw() +
  theme(panel.grid = element_blank())
```

### Many Categories
```{r}
library(rethinking)
data(milk)
d <- milk
rm(milk)
detach(package:rethinking, unload = T)
library(brms)
```



```{r}
library(recipes)

(d2 <- recipe(kcal.per.g ~ 1 + clade, d) %>% 
  step_dummy(clade, one_hot = T) %>% 
  prep(d) %>% 
  bake(d)) %>% 
  glimpse
```

```{r}
b5.16 <- 
  brm(data = d2, family = gaussian,
      kcal.per.g ~ 0 + clade_Ape + clade_New.World.Monkey + clade_Old.World.Monkey + clade_Strepsirrhine,
      prior = c(prior(normal(0, 1), class = b),
                prior(uniform(0, 10), class = sigma)),
      iter = 2000, warmup = 500, chains = 4, cores = 4,
      control = list(adapt_delta = 0.8))
plot(b5.16)
```

```{r fig.height=1, fig.width=3.5}
b5.16 %>%
  posterior_samples %>% 
  select(starts_with("b_"), starts_with("lp__")) %>% 
  tidy_coef_plot()
```


```{r}
b5.16_alt <- 
  brm(data = d, family = gaussian,
      kcal.per.g ~ 0 + clade,
      prior = c(prior(normal(.6, 10), class = b),
                prior(uniform(0, 10), class = sigma)),
      iter = 2000, warmup = 500, chains = 4, cores = 4,
      refresh = 0)
print(b5.16_alt)

```












