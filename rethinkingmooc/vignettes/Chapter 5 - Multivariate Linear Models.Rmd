---
title: "Chapter 5 - Multivariate Linear Models"
output: html_notebook
---

# Introduction

In large data sets, every pair of variables has a statistically discernible non-zero correlation. Most of these relationships are non-causal. Multivariate regression can help us examine causal relatinoships by:

1. Statistical control for confounders. A counfound is a variable that may be correlated with another variable of interest.
2. Multiple Cuasation. A phenomonen may really arise from multiple causes.
3. Interactions. Even when variables are completely uncorrelated, the importance of each may still depend upon the other.

# Spurious association
In statistics, a spurious relationship or spurious correlation is a mathematical relationship in which two or more events or variables are not causally related to each other, yet it may be wrongly inferred that they are, due to either coincidence or the presence of a certain third, unseen factor referred to as a `confounding factor`.

In this example, we'll examine `divorce rate`, `median age at marriage`, and `marriage rate` in a data set where each observation is a state. 
```{r}
library(rethinking)
library(tidyverse)
library(rethinkingmooc)
rstan_options(auto_write = TRUE)

data(WaffleDivorce)
d <- WaffleDivorce
d %>% 
  skimr::skim()
```



```{r}
# Standardize the predictor `median age at marriage`
d$MedianAgeMarriage.s <- (d$MedianAgeMarriage - mean(d$MedianAgeMarriage)) / sd(d$MedianAgeMarriage)

# Fit divorce rate 
m5.1 <- rethinking::map(
  alist(
    Divorce ~ dnorm(mu, sigma),
    mu <- a + bA * MedianAgeMarriage.s,
    a ~ dnorm(10, 10),
    bA ~ dnorm(0, 1),
    sigma ~ dunif(0, 10)), 
  data = d)

# Standardize the predictor `Marriage`
d$Marriage.s <- (d$Marriage - mean(d$Marriage)) / sd(d$Marriage)

# Fit it's relationship with divorce rate
m5.2 <- rethinking::map(
  alist(
    Divorce ~ dnorm(mu, sigma),
    mu <- a + bR * Marriage.s,
    a ~ dnorm(10, 10),
    bR ~ dnorm(0, 1),
    sigma ~ dunif(0, 10)),
  data = d)

par(mfrow=c(1,2)) 
m5.1 %>% 
  precis %>% 
  plot(main="Median age at marriage")
m5.2 %>% 
  precis %>% 
  plot(main = "Marriage rate")
```

The regression above shows that each additional standard deviation of delay in marriage (1.24 years) predicts a decrease of about one divorce per thousand adults, withh an 89% interval from -1.4 to -0.7. It also shows an increase of .6 divorces for every additional standard deviation of marriage rate (3.8). However, merely comparing paramter means between different bivariate regressions tells us very little. Both predictors could provide independent value, be redundant, or eliminate the value of the other. 

What we should do is model them together in a multivariate model. We want to know, `What is the predictive value of a variable once I already know all of the other predictor variables.`

```{r}
m5.3 <- rethinking::map(
  alist(
    Divorce ~ dnorm(mu, sigma),
    mu <- a + bR*Marriage.s + bA*MedianAgeMarriage.s,
    a ~ dnorm(10, 10),
    bR ~ dnorm(0, 1),
    bA ~ dnorm(0, 1),
    sigma ~ dunif(0, 10)),
  data = d)

m5.3 %>% 
  precis %>% 
  plot
```

The posterior mean for marginal effect of state's `marraige rate` on `divorce rate`, `bR`, is now close to zero. Once we know median age at marriage for a state, there is little or no additional predictive power in also knowing the rate of marriage in that state.

# Plotting multivariate posteriors

Multivariate regressions can get complicated. To interrogate them better, we can use plots to our advantage. There are some plot types to interpret models.

1. _Predictor residual plots_: Show the outcome against residual predictor values.
2. _Counterfactual plots_: Show implied predictions for made up data.
3. _Posterior prediction plots_: Model based predictions against raw data.

## Predictor residual plots

A predictor variable residual is the average prediction error when we use all of the other predictor variables to model a predictor of interest. This allows us to visualize the bivariate relationship between the predictor and the outcome while "controlling" for all of the other predictor variables.

```{r}
# Model marriage as a function of Median Marriage Age
m5.4 <- rethinking::map(
  alist(
    Marriage.s ~ dnorm(mu, sigma),
    mu <- a + b*MedianAgeMarriage.s,
    a ~ dnorm(0, 10),
    b ~ dnorm(0, 1),
    sigma ~ dunif(0, 10)),
  data = d)

# Compute residuals
mu.hat <- coef(m5.4)['a'] + coef(m5.4)['b']*d$MedianAgeMarriage.s
m.resid <- d$Marriage.s - mu.hat

# Generate predictor residual plot
data_frame(
  divorce_rate = d$MedianAgeMarriage.s,
  marriage_rate = m.resid
) %>% 
  ggplot(aes(marriage_rate, divorce_rate)) +
    #geom_point(shape=1, size=3) +
    geom_point(colour="steelblue", size=3, alpha=I(1/3)) +
    geom_smooth(method=lm, formula = y ~ poly(x, 1), colour="black") +
    geom_vline(xintercept=0, linetype=2) +
    theme_bw() + theme_empty() +
    ggtitle("Predictor residual plot") +
    xlab("Marriage Rate Residuals") + ylab("Divorce Rate") + 
    annotate("text", x=-.21, y=3, label = "Slower") +
    annotate("text", x=.2, y=3, label = "Faster")

```

Above, we plot the residuals from using median age at marriage to predict marriage rate against our outcome of interest. This plot displays the linear relationship between `divorce` and `marriage rates`, having statistically "controlled" for `median age of marriage`. Recall that multiple linear regression models measure the remaining association of each predictor with the outcome. By computing the predictor residual plots you perform those calculations yourself. 

What does it mean for residuals to be explained well by another variables?

## Counterfactual plot

In counterfactual plots, we hold all predictor variables to their mean value except for one in particular. We plot this one varying predictor against our outcome of interest. We can generate this for each predictor variable independently.

```{r}
# Prepare counterfactual data
age_avg <- mean(d$MedianAgeMarriage.s)
new_marriage_rate <- seq(from = -3, to = 3, length.out = 30)
new_data <- data_frame(
  Marriage.s = new_marriage_rate,
  MedianAgeMarriage.s = age_avg)

# Calculate mean divorce (mu) and percentile interval
mu <- link(m5.3, data = new_data)
mu_mean <- apply(mu, 2, mean)
mu_pi <- apply(mu, 2, PI)

# Simulate counterfactual divorce outcomes and percentie interval
divrate_sim <- sim(m5.3, data = new_data, n = 1e4)
divrate_pi <- apply(divrate_sim, 2, PI)

# counterfactual plot
data_frame(
  new_marriage_rate,
  mean_divorce_rate = mu_mean,
  mean_divorce_rate_min = mu_pi[1,],
  mean_divorce_rate_max = mu_pi[2,],
  divorce_rate_min = divrate_pi[1,],
  divorce_rate_max = divrate_pi[2,]) %>% 
  
  ggplot(aes(x=new_marriage_rate, y=mean_divorce_rate)) +
    geom_line() +
    geom_ribbon(aes(ymin = mean_divorce_rate_min, ymax = mean_divorce_rate_max), alpha = I(1/5)) +
    geom_ribbon(aes(ymin = divorce_rate_min, ymax = divorce_rate_max), alpha = I(1/5)) +
    coord_cartesian(xlim=c(-2, 3), ylim=c(6, 13)) +
    ylab("Divorce") + xlab("Marriage Rate") +
    ggtitle("Counterfactual plot", "MedianAgeMarriage.s = 0") +
    theme_empty() + geom_vline(xintercept = 0, linetype=2) +
    annotate("text", -.25, 13, label="Slower") +
    annotate("text", .3, 13, label="Faster")
```

## Posterior prediction plots

This is just a plot of predicted vs actual.

1. Did the model fit correctly?
2. How does the model fail?

```{r}
# Run link on training data
mu <- link(m5.3)

# Summarize the samples
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI)

# Simulate observations
divorce.sim <- sim(m5.3, n=1e4)
divorce.PI <- apply(divorce.sim, 2, PI)

# Plot predicted against actual
data_frame(x = d$Divorce,
           y = mu.mean,
           ymin=mu.PI[1,],
           ymax=mu.PI[2,]) %>% 
    ggplot(aes(x, y)) + 
      geom_point(shape = 1, size = 2) +
      geom_linerange(aes(ymin = ymin, ymax = ymax)) +
      geom_abline(intercept = 0, slope = 1, linetype=2) + 
      xlab("Observed Divorce") + ylab("Predictived Divorce") +
      ggtitle("Posterior prediction plot")
```

## Revisiting spurious predictors
Lets create an explicit example of spurious prediction. Below, `x_real` is the actual causal parameter of interest but it is correlated with a spurious variable, `x_spur` but it is correlated with both y and x_spur
```{r}
N <- 100
df <- data_frame(x_real = rnorm(N),
                 y = rnorm(N, x_real),
                 x_spur = rnorm(N, x_real))
ggplot(df, aes(y=y)) +
  geom_smooth(aes(x=x_spur), method=lm, colour="black") +
  xlab("A spurious variable") + ggtitle("An example of spurious association") +
  theme_empty()

```

# Masked relationship

Bivariate relationships can be misleading when two predictor variables are correlated with each other but one variable is positively correlated with the outcome and the other is negatively correlated with it. Below, we will simulate data in which two meaningful predictors act to mask one another. 

```{r}
# Required for pair plot
library(GGally)

# number of cases
N <- 100
# correlation b/w x_pos and x_neg
rho <- .7  

df = data_frame(x_pos = rnorm(N),
                x_neg = rnorm(N, rho*x_pos, 
                              sqrt(1-rho^2)),
                y = rnorm(N , x_pos - x_neg))

GGally::ggpairs(df)
```

It looks like Y has nothing to do with either x_pos or x_neg, however, note that Y is a function of both x_pos and x_neg. A multivariate regression would pick up on this exactly.
```{r}
fit_fixed <- rethinking::map(
  alist(
    y ~ dnorm(mu, sigma),
    mu <- a + b*x_pos + c*x_neg,
    a ~ dnorm(0, 10),
    b ~ dnorm(0, 10),
    c ~ dnorm(0, 10),
    sigma ~ dunif(0, 10)),
  data = as.data.frame(df))

plot(precis(fit_fixed))
```

Simple bivariate regressions would miss this entirely.
```{r}
fit_masked <- rethinking::map(
  alist(
    y ~ dnorm(mu, sigma),
    mu <- a + b*x_pos,
    a ~ dnorm(0, 10),
    b ~ dnorm(0, 10),
    sigma ~ dunif(0, 10)),
  data = as.data.frame(df))

plot(precis(fit_masked))
```

# When adding variables hurts

There are a few reasons why you don't just want to chuck every variable you have in to your data frame.

1. *Multicollinearity*: Very strong correlation between two or more predictor variables. Multiple linear regression answers the question, "`What is the value of knowing each predictor, after already knowing all of the other predictors?`". In cases where variables are correlated, a single variables estimate will be deflated as its effect on the outcome is already being explained by another variable.
2. *Post treatment bias*: 
3. *Overfitting*:

## Multicollinearity

Multiple linear regression answers the question, "`What is the value of knowing each predictor, after already knowing all of the other predictors?`". In cases where variables are correlated, a single variables estimate will be deflated as its effect on the outcome is already being explained by another variable. As an example, below we regress height as a function of left and right leg length.
```{r}
N <- 100

df <- data_frame(
  # Each individual's height
  height = rnorm(N, 10, 2),
  # Leg as proportion of height
  leg_prop = runif(N, .4, .5),
  # left leg as proportion + error
  leg_left = leg_prop * height + rnorm(N, 0, .02),
  # righ tleg as proportion + error
  leg_right = leg_prop * height + rnorm(N, 0, .02)) %>% 
  select(height, leg_left, leg_right)

m5.8 <- rethinking::map(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + bl*leg_left + br*leg_right,
    a ~ dnorm(10, 100),
    bl ~ dnorm(2, 10),
    br ~ dnorm(2, 10),
    sigma ~ dunif(0, 10)
  ), data=as.data.frame(df))
plot(precis(m5.8))
```

When we ask the question, `What is the value of knowing each leg's length, after already knowing the other leg's length?`, the results in the plot above should be intuitive.

Recall that the posterior distribution samples every possible combination of parameters according to their plausibilities conditional on the model specification, the data, and the prior. Sampling from the posterior and plotting left and right leg length against each other shows how correlated they are.

```{r}
post <- extract.samples(m5.8, n=1e4)
ggplot(post, aes(bl, br)) + 
  geom_bin2d(bins=40) + theme_empty() +
  xlab("left leg") + ylab("right leg") +
  ggtitle("Posterior sample", "These two predictors are highly correlated")
```

In summary, including two predictor variables that are strongly correlated in your model can lead to confusion. If all you're interested in is prediction though, you have nothing to worry about.

## Post treatment bias


# Categorical variables

# Ordinary least squares and lm






















